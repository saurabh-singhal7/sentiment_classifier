{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from utils import *\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Dropout, LSTM, Activation\n",
    "from keras.layers import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.initializers import glorot_uniform\n",
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load and clean the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"sentiment_data/train.csv\")\n",
    "test_data = pd.read_csv(\"sentiment_data/test.csv\")\n",
    "train_data = train_data.dropna()\n",
    "test_data = test_data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data[['selected_text','sentiment']]\n",
    "test_data = test_data[['text','sentiment']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_y = train_data['selected_text'], train_data['sentiment']\n",
    "test_x, test_y = test_data['text'], test_data['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = train_y.to_numpy()\n",
    "test_y = test_y.to_numpy()\n",
    "for i in range(len(train_y)):\n",
    "    if train_y[i] == 'positive':\n",
    "        train_y[i] = 0\n",
    "    elif train_y[i] == 'negative':\n",
    "        train_y[i] = 1\n",
    "    else:\n",
    "        train_y[i] = 2\n",
    "for i in range(len(test_y)):\n",
    "    if test_y[i] == 'positive':\n",
    "        test_y[i] = 0\n",
    "    elif test_y[i] == 'negative':\n",
    "        test_y[i] = 1\n",
    "    else:\n",
    "        test_y[i] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_temp = train_y.copy()\n",
    "y_test_temp = test_y.copy()\n",
    "train_y = to_categorical(train_y)\n",
    "test_y = to_categorical(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " ...\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]]\n",
      "[[0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " ...\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(train_y)\n",
    "print(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train_x.to_numpy()\n",
    "test_x = test_x.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(train_x)):\n",
    "    train_x[i] = depure_data(train_x[i])\n",
    "for i in range(len(test_x)):\n",
    "    test_x[i] = depure_data(test_x[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "print(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "regular_punct = list(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(train_x)):\n",
    "    train_x[i] = remove_punctuation(train_x[i], regular_punct)\n",
    "for i in range(len(test_x)):\n",
    "    test_x[i] = remove_punctuation(test_x[i], regular_punct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27480,)\n",
      "['Last session of the day'\n",
      " 'Shanghai is also really exciting precisely  skyscrapers galore Good tweeps in China SH BJ'\n",
      " 'Recession hit Veronique Branquinho she has to quit her company such a shame'\n",
      " ...\n",
      " 'I know what you mean My little dog is sinking into depression he wants to move someplace tropical'\n",
      " 'sutra what is your next youtube video gonna be about I love your videos'\n",
      " 'omgssh ang cute ng bby']\n"
     ]
    }
   ],
   "source": [
    "print(train_x.shape)\n",
    "print(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "went 2 see the hannah montana movie wiv jodie on friday and it was WELL GOOD but i feel nasty for laughing at the lil girl that cryed LOL\n"
     ]
    }
   ],
   "source": [
    "print(max(test_x, key=len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(max(train_x, key=len))\n",
    "# maxLen = len(max(train_x, key=len).split())\n",
    "# print(maxLen)\n",
    "maxLen = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index, index_to_word, word_to_vec_map = read_glove_vecs('glove.6B.50d.txt\\glove.6B.50d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the index of unk in the vocabulary is 372306\n",
      "the 98670th word in the vocabulary is chiaramonte\n"
     ]
    }
   ],
   "source": [
    "# testing embeddings\n",
    "word = \"unk\"\n",
    "idx = 98670\n",
    "print(\"the index of\", word, \"in the vocabulary is\", word_to_index[word])\n",
    "print(\"the\", str(idx) + \"th word in the vocabulary is\", index_to_word[idx])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentences_to_indices(X, word_to_index, max_len):\n",
    "    \"\"\"\n",
    "    Converts an array of sentences (strings) into an array of indices corresponding to words in the sentences.\n",
    "    The output shape should be such that it can be given to `Embedding()`. \n",
    "    \n",
    "    Arguments:\n",
    "    X -- array of sentences (strings), of shape (m, 1)\n",
    "    word_to_index -- a dictionary containing the each word mapped to its index\n",
    "    max_len -- maximum number of words in a sentence. You can assume every sentence in X is no longer than this. \n",
    "    \n",
    "    Returns:\n",
    "    X_indices -- array of indices corresponding to words in the sentences from X, of shape (m, max_len)\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[0]                                   # number of training examples\n",
    "    \n",
    "    # Initialize X_indices as a numpy matrix of zeros and the correct shape\n",
    "    X_indices = np.zeros((m, max_len))\n",
    "    \n",
    "    for i in range(m):                               # loop over training examples\n",
    "                \n",
    "        # Convert the ith training sentence in lower case and split is into words. You should get a list of words.\n",
    "        sentence_words = X[i].lower().split()\n",
    "                \n",
    "        # Initialize j to 0\n",
    "        j = 0\n",
    "        \n",
    "        # Loop over the words of sentence_words\n",
    "        for w in sentence_words:\n",
    "            \n",
    "            # Set the (i,j)th entry of X_indices to the index of the correct word.\n",
    "            if w not in word_to_index:\n",
    "                X_indices[i, j] = word_to_index[\"unk\"]\n",
    "            else:\n",
    "                X_indices[i, j] = word_to_index[w]\n",
    "\n",
    "            # Increment j to j + 1\n",
    "            j = j + 1\n",
    "    \n",
    "    return X_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrained_embedding_layer(word_to_vec_map, word_to_index):\n",
    "    \"\"\"\n",
    "    Creates a Keras Embedding() layer and loads in pre-trained GloVe 50-dimensional vectors.\n",
    "    \n",
    "    Arguments:\n",
    "    word_to_vec_map -- dictionary mapping words to their GloVe vector representation.\n",
    "    word_to_index -- dictionary mapping from words to their indices in the vocabulary (400,001 words)\n",
    "\n",
    "    Returns:\n",
    "    embedding_layer -- pretrained layer Keras instance\n",
    "    \"\"\"\n",
    "    \n",
    "    vocab_len = len(word_to_index) + 1                  # adding 1 to fit Keras embedding (requirement)\n",
    "    emb_dim = word_to_vec_map[\"cucumber\"].shape[0]      # define dimensionality of your GloVe word vectors (= 50)\n",
    "    \n",
    "    # Initialize the embedding matrix as a numpy array of zeros.\n",
    "    emb_matrix = np.zeros((vocab_len, emb_dim))\n",
    "   \n",
    "    # Set each row \"idx\" of the embedding matrix to be \n",
    "    # the word vector representation of the idx'th word of the vocabulary\n",
    "    for word, index in word_to_index.items():\n",
    "        emb_matrix[index, :] = word_to_vec_map[word]\n",
    "\n",
    "    # Define Keras embedding layer with the correct input and output sizes\n",
    "    # Make it non-trainable.\n",
    "    embedding_layer = Embedding(vocab_len, emb_dim, trainable=False)\n",
    "    \n",
    "    # Build the embedding layer, it is required before setting the weights of the embedding layer. \n",
    "    embedding_layer.build((None,))\n",
    "    \n",
    "    # Set the weights of the embedding layer to the embedding matrix. Your layer is now pretrained.\n",
    "    embedding_layer.set_weights([emb_matrix])\n",
    "    \n",
    "    return embedding_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights[0][1][3] = -0.3403\n"
     ]
    }
   ],
   "source": [
    "embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)\n",
    "print(\"weights[0][1][3] =\", embedding_layer.get_weights()[0][1][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentinial_classifier(input_shape, word_to_vec_map, word_to_index):\n",
    "    \"\"\"\n",
    "    Function creating the Emojify-v2 model's graph.\n",
    "    \n",
    "    Arguments:\n",
    "    input_shape -- shape of the input, usually (max_len,)\n",
    "    word_to_vec_map -- dictionary mapping every word in a vocabulary into its 50-dimensional vector representation\n",
    "    word_to_index -- dictionary mapping from words to their indices in the vocabulary (400,001 words)\n",
    "\n",
    "    Returns:\n",
    "    model -- a model instance in Keras\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define sentence_indices as the input of the graph\n",
    "    # It should be of shape input_shape and dtype 'int32' (as it contains indices).\n",
    "    sentence_indices = Input(input_shape, dtype='int32')\n",
    "    \n",
    "    # Create the embedding layer pretrained with GloVe Vectors (â‰ˆ1 line)\n",
    "    embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)\n",
    "    \n",
    "    # Propagate sentence_indices through your embedding layer, you get back the embeddings\n",
    "    embeddings = embedding_layer(sentence_indices)    \n",
    "    \n",
    "    # Propagate the embeddings through an LSTM layer with 128-dimensional hidden state\n",
    "    X = LSTM(128, return_sequences=True)(embeddings)\n",
    "    # Add dropout with a probability of 0.5\n",
    "    X = Dropout(0.5)(X)\n",
    "    # Propagate X trough another LSTM layer with 128-dimensional hidden state\n",
    "    X = LSTM(128, return_sequences=False)(X)\n",
    "    # Add dropout with a probability of 0.5\n",
    "    X = Dropout(0.5)(X)\n",
    "    # Propagate X through a Dense layer with softmax activation to get back a batch of 3-dimensional vectors.\n",
    "    X = Dense(3)(X)\n",
    "    # Add a softmax activation\n",
    "    X = Activation('softmax')(X)\n",
    "    \n",
    "    # Create Model instance which converts sentence_indices into X.\n",
    "    model = Model(inputs=sentence_indices, outputs=X)\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 40)]              0         \n",
      "                                                                 \n",
      " embedding_1 (Embedding)     (None, 40, 50)            20000050  \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 40, 128)           91648     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 40, 128)           0         \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 128)               131584    \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 3)                 387       \n",
      "                                                                 \n",
      " activation (Activation)     (None, 3)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 20,223,669\n",
      "Trainable params: 223,619\n",
      "Non-trainable params: 20,000,050\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = sentinial_classifier((maxLen,), word_to_vec_map, word_to_index)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_indices = sentences_to_indices(train_x, word_to_index, maxLen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[185457. 390139. 174642. ...      0.      0.      0.]\n",
      " [337670. 315103.      0. ...      0.      0.      0.]\n",
      " [ 86656. 239105.      0. ...      0.      0.      0.]\n",
      " ...\n",
      " [393223. 164328. 151349. ...      0.      0.      0.]\n",
      " [ 87775. 193716. 383514. ...      0.      0.      0.]\n",
      " [ 51582. 358160. 150110. ...      0.      0.      0.]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/35\n",
      "859/859 [==============================] - 42s 45ms/step - loss: 0.6828 - accuracy: 0.7207\n",
      "Epoch 2/35\n",
      "859/859 [==============================] - 42s 49ms/step - loss: 0.5616 - accuracy: 0.7892\n",
      "Epoch 3/35\n",
      "859/859 [==============================] - 38s 45ms/step - loss: 0.5123 - accuracy: 0.8076\n",
      "Epoch 4/35\n",
      "859/859 [==============================] - 36s 42ms/step - loss: 0.4881 - accuracy: 0.8176\n",
      "Epoch 5/35\n",
      "859/859 [==============================] - 37s 43ms/step - loss: 0.4609 - accuracy: 0.8267\n",
      "Epoch 6/35\n",
      "859/859 [==============================] - 42s 49ms/step - loss: 0.4376 - accuracy: 0.8364\n",
      "Epoch 7/35\n",
      "859/859 [==============================] - 41s 47ms/step - loss: 0.4239 - accuracy: 0.8433\n",
      "Epoch 8/35\n",
      "859/859 [==============================] - 37s 44ms/step - loss: 0.4013 - accuracy: 0.8498\n",
      "Epoch 9/35\n",
      "859/859 [==============================] - 41s 48ms/step - loss: 0.3852 - accuracy: 0.8585\n",
      "Epoch 10/35\n",
      "859/859 [==============================] - 39s 45ms/step - loss: 0.3690 - accuracy: 0.8619\n",
      "Epoch 11/35\n",
      "859/859 [==============================] - 43s 50ms/step - loss: 0.3592 - accuracy: 0.8678\n",
      "Epoch 12/35\n",
      "859/859 [==============================] - 43s 50ms/step - loss: 0.3325 - accuracy: 0.8799\n",
      "Epoch 13/35\n",
      "859/859 [==============================] - 45s 53ms/step - loss: 0.3186 - accuracy: 0.8861\n",
      "Epoch 14/35\n",
      "859/859 [==============================] - 48s 56ms/step - loss: 0.3017 - accuracy: 0.8923\n",
      "Epoch 15/35\n",
      "859/859 [==============================] - 40s 47ms/step - loss: 0.2820 - accuracy: 0.9016\n",
      "Epoch 16/35\n",
      "859/859 [==============================] - 41s 48ms/step - loss: 0.2728 - accuracy: 0.9035\n",
      "Epoch 17/35\n",
      "859/859 [==============================] - 46s 53ms/step - loss: 0.2523 - accuracy: 0.9125\n",
      "Epoch 18/35\n",
      "859/859 [==============================] - 38s 44ms/step - loss: 0.2407 - accuracy: 0.9167\n",
      "Epoch 19/35\n",
      "859/859 [==============================] - 39s 45ms/step - loss: 0.2195 - accuracy: 0.9255\n",
      "Epoch 20/35\n",
      "859/859 [==============================] - 39s 46ms/step - loss: 0.2144 - accuracy: 0.9275\n",
      "Epoch 21/35\n",
      "859/859 [==============================] - 47s 55ms/step - loss: 0.1978 - accuracy: 0.9338\n",
      "Epoch 22/35\n",
      "859/859 [==============================] - 41s 47ms/step - loss: 0.1916 - accuracy: 0.9337\n",
      "Epoch 23/35\n",
      "859/859 [==============================] - 45s 53ms/step - loss: 0.1828 - accuracy: 0.9398\n",
      "Epoch 24/35\n",
      "859/859 [==============================] - 39s 45ms/step - loss: 0.1664 - accuracy: 0.9455\n",
      "Epoch 25/35\n",
      "859/859 [==============================] - 43s 50ms/step - loss: 0.1600 - accuracy: 0.9484\n",
      "Epoch 26/35\n",
      "859/859 [==============================] - 44s 52ms/step - loss: 0.1510 - accuracy: 0.9507\n",
      "Epoch 27/35\n",
      "859/859 [==============================] - 40s 47ms/step - loss: 0.1494 - accuracy: 0.9484\n",
      "Epoch 28/35\n",
      "859/859 [==============================] - 42s 48ms/step - loss: 0.1424 - accuracy: 0.9513\n",
      "Epoch 29/35\n",
      "859/859 [==============================] - 47s 55ms/step - loss: 0.1310 - accuracy: 0.9563\n",
      "Epoch 30/35\n",
      "859/859 [==============================] - 42s 49ms/step - loss: 0.1292 - accuracy: 0.9568\n",
      "Epoch 31/35\n",
      "859/859 [==============================] - 51s 59ms/step - loss: 0.1202 - accuracy: 0.9597\n",
      "Epoch 32/35\n",
      "859/859 [==============================] - 48s 56ms/step - loss: 0.1143 - accuracy: 0.9610\n",
      "Epoch 33/35\n",
      "859/859 [==============================] - 48s 55ms/step - loss: 0.1087 - accuracy: 0.9634\n",
      "Epoch 34/35\n",
      "859/859 [==============================] - 46s 53ms/step - loss: 0.1096 - accuracy: 0.9622\n",
      "Epoch 35/35\n",
      "859/859 [==============================] - 46s 53ms/step - loss: 0.1143 - accuracy: 0.9614\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x225db613d00>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_indices, train_y, epochs = 35, batch_size = 32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111/111 [==============================] - 2s 16ms/step - loss: 2.3457 - accuracy: 0.5951\n",
      "\n",
      "Test accuracy =  0.5950763821601868\n"
     ]
    }
   ],
   "source": [
    "X_test_indices = sentences_to_indices(test_x, word_to_index, max_len = maxLen)\n",
    "loss, acc = model.evaluate(X_test_indices, test_y)\n",
    "print()\n",
    "print(\"Test accuracy = \", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 20ms/step\n",
      "hi how are you 2\n"
     ]
    }
   ],
   "source": [
    "# Change the sentence below to see your prediction. Make sure all the words are in the Glove embeddings.  \n",
    "x_test = np.array(['hi how are you'])\n",
    "X_test_indices = sentences_to_indices(x_test, word_to_index, maxLen)\n",
    "print(x_test[0] +' '+ output_to_sentiment(np.argmax(model.predict(X_test_indices))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
